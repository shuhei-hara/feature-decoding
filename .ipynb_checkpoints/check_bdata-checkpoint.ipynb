{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bc5655-7c3c-4580-bc80-c21174a6e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import bdpy\n",
    "from bdpy.mri import create_bdata_fmriprep, add_rois, merge_rois, add_hcp_rois, add_hcp_visual_cortex\n",
    "from bdpy.preproc import average_sample, reduce_outlier, regressout, shift_sample\n",
    "\n",
    "subject = 'DI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eadec5-85e2-4b61-9f62-509300e0d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_dir = '/bucket/DoyaU/Shuhei/decoding_visual/BIDS_training'\n",
    "\n",
    "# The output bdata will be saved here.\n",
    "\n",
    "output_dir = '/flash/DoyaU/shuhei/bdata_decoding'\n",
    "output_data_type_list = ['volume_standard']\n",
    "\n",
    "exclude = {'run': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]}\n",
    "roi_mask_standard_dir = '/bucket/DoyaU/Shuhei/decoding_visual/ROI_standard'\n",
    "rois_files = {\n",
    "    'volume_standard':           [os.path.join(roi_mask_standard_dir, '*.nii.gz')],\n",
    "}\n",
    "\n",
    "label_mapper = {'stimulus_name' : '/bucket/DoyaU/Shuhei/decoding_visual/label_mapping.csv'\n",
    "}\n",
    "\n",
    "split_task_label = True\n",
    "\n",
    "# Preprocessing parameters ---------------------------------------------------\n",
    "\n",
    "shift_size = 2  # Num of volumes (not seconds) to be shifted\n",
    "\n",
    "# Main #######################################################################\n",
    "\n",
    "if bids_dir is None:\n",
    "    bids_dir = '../bids'\n",
    "\n",
    "bids_dir = os.path.abspath(bids_dir)\n",
    "\n",
    "data_id = os.path.basename(os.path.dirname(bids_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82ecc06d-a89a-4eb3-a3cb-7219bbe3bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Data ID:        decoding_visual\n",
      "BIDS directory: /bucket/DoyaU/Shuhei/decoding_visual/BIDS_training\n",
      "\n",
      "----------------------------------------\n",
      "Loading /bucket/DoyaU/Shuhei/decoding_visual/BIDS_training\n",
      "BIDS data path: /bucket/DoyaU/Shuhei/decoding_visual/BIDS_training\n",
      "----------------------------------------\n",
      "Subject: sub-DI\n",
      "\n",
      "Task:    task-decodertraining\n",
      "Session: 1 (session)\n",
      "Data: volume_standard\n",
      "\n",
      "Run 1\n",
      "EPI:             /bucket/DoyaU/Shuhei/decoding_visual/fmriprep/sub-DI/func/sub-DI_task-decodertraining_run-001_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\n",
      "Task event file: sub-DI/func/sub-DI_task-decodertraining_run-001_events.tsv\n",
      "Confounds file:  /bucket/DoyaU/Shuhei/decoding_visual/fmriprep/sub-DI/func/sub-DI_task-decodertraining_run-001_desc-confounds_timeseries.tsv\n",
      "\n",
      "\n",
      "All data loaded\n"
     ]
    }
   ],
   "source": [
    "output_data_type = output_data_type_list[0]\n",
    "brain_data_key = 'VoxelData'\n",
    "output_data_type_fix = output_data_type\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Data ID:        %s' % data_id)\n",
    "print('BIDS directory: %s' % bids_dir)\n",
    "print('')\n",
    "\n",
    "# Load data ------------------------------------------------------------------\n",
    "print('----------------------------------------')\n",
    "print('Loading %s' % bids_dir)\n",
    "bdata_list, data_labels = create_bdata_fmriprep(bids_dir,\n",
    "                                                fmriprep_dir='/bucket/DoyaU/Shuhei/decoding_visual',\n",
    "                                                data_mode=output_data_type_fix,\n",
    "                                                label_mapper=label_mapper,\n",
    "                                                exclude=exclude,\n",
    "                                                split_task_label=split_task_label,\n",
    "                                                with_confounds=True,\n",
    "                                                return_data_labels=True,\n",
    "                                                return_list=True,\n",
    "                                                subject=subject)\n",
    "print('')\n",
    "print('All data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c402077-d1ab-4f66-8a68-d46c1feca55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Data DI_decodertraining\n",
      "Num columns: 599950\n",
      "Num sample:  239\n",
      "----------------------------------------\n",
      "Adding ROIs\n"
     ]
    }
   ],
   "source": [
    "for bdata, data_label in zip(bdata_list, data_labels):\n",
    "\n",
    "    # Misc fix on data_label\n",
    "    data_label = re.sub('^sub-', '', data_label)\n",
    "    data_label = re.sub('_task-', '_', data_label)\n",
    "\n",
    "    # Disp info\n",
    "    print('----------------------------------------')\n",
    "    print('Data %s' % data_label)\n",
    "    print('Num columns: %d' % bdata.dataset.shape[1])\n",
    "    print('Num sample:  %d' % bdata.dataset.shape[0])\n",
    "    print('----------------------------------------')\n",
    "    print('Adding ROIs')\n",
    "\n",
    "    if output_data_type in ['volume_native', 'volume_native_resampled', 'volume_standard']:\n",
    "        # Volume data\n",
    "        data_type = 'volume'\n",
    "\n",
    "    bdata = add_rois(bdata, rois_files[output_data_type], data_type=data_type, prefix_map=roi_prefix_mapper)\n",
    "    \n",
    "    # print([k for k in bdata.metadata.key if 'V1' in k or 'ROI' in k])\n",
    "\n",
    "    # # それぞれの値の確認\n",
    "    # roi_flag = bdata.get_metadata('ROI_standard_V1')  # 例\n",
    "    # print(roi_flag)\n",
    "\n",
    "\n",
    "    # print('----------------------------------------')\n",
    "    # print('Saving raw (unpreprocessed) BData')\n",
    "    # save_file = os.path.join(output_dir, data_label + '_fmap_' + output_data_type + '_raw' + '.h5')\n",
    "    # bdata.save(save_file)\n",
    "    # print('Saved %s' % save_file)\n",
    "    # print('')\n",
    "\n",
    "    # print('Raw data size: %d x %d' % (bdata.dataset.shape[0], bdata.dataset.shape[1]))\n",
    "    # print('')\n",
    "\n",
    "    # # Preprocessing --------------------------------------------------------------\n",
    "    # print('----------------------------------------')\n",
    "    # print('Preprocessing')\n",
    "    # print('')\n",
    "\n",
    "    # # Shift data\n",
    "    # runs = bdata.get('Run').flatten()\n",
    "    # bdata.applyfunc(shift_sample, where=[brain_data_key, 'MotionParameter', 'Confounds'], group=runs,\n",
    "    #                 shift_size=shift_size)\n",
    "\n",
    "    # # Motion regressout, mean substraction, and linear detrending\n",
    "    # runs = bdata.get('Run').flatten()\n",
    "    # motionparam = bdata.get('MotionParameter')\n",
    "    # bdata.applyfunc(regressout, where=brain_data_key, group=runs,\n",
    "    #                 regressor=motionparam,\n",
    "    #                 remove_dc=True, linear_detrend=True)\n",
    "\n",
    "    # # Outlier reduction\n",
    "    # runs = bdata.get('Run').flatten()\n",
    "    # bdata.applyfunc(reduce_outlier, where=brain_data_key, group=runs,\n",
    "    #                 std=True, std_threshold=3, n_iter=10,\n",
    "    #                 maxmin=False)\n",
    "\n",
    "    # # Within-block averaging\n",
    "    # blocks = bdata.get('Block').flatten()\n",
    "    # bdata.applyfunc(average_sample, where=brain_data_key, group=blocks)\n",
    "\n",
    "    # # Remove one-back repetition blocks\n",
    "    # trialtype = bdata.get('trial_type').flatten()\n",
    "    # # trialtype = bdata.get('stim_file').flatten()\n",
    "    # print('trialtype', trialtype)\n",
    "    # bdata.dataset = np.delete(bdata.dataset, np.where(trialtype == 2), axis=0)\n",
    "\n",
    "    # # Remove rest blocks\n",
    "    # trialtype = bdata.get('trial_type').flatten()\n",
    "    # # trialtype = bdata.get('stim_file').flatten()\n",
    "    # bdata.dataset = np.delete(bdata.dataset, np.where(trialtype < 0), axis=0)\n",
    "\n",
    "    # # Save preprocessed BData ----------------------------------------------------\n",
    "    # print('----------------------------------------')\n",
    "    # print('Saving preprocessed BData')\n",
    "    # save_file = os.path.join(output_dir, data_label + '_fmap_' + output_data_type + '_prep' + '.h5')\n",
    "    # bdata.save(save_file)\n",
    "    # print('Saved %s' % save_file)\n",
    "    # print('')\n",
    "\n",
    "    # print('Preprocessed data size: %d x %d' % (bdata.dataset.shape[0], bdata.dataset.shape[1]))\n",
    "    # print('')\n",
    "\n",
    "    # del bdata\n",
    "    # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a27bec62-7af2-442d-a399-de78e55d1560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  4.,  4.,\n",
       "        4.,  4.,  5.,  5.,  5.,  5.,  6.,  6.,  6.,  6.,  7.,  7.,  7.,\n",
       "        7.,  8.,  8.,  8.,  8.,  9.,  9.,  9.,  9., 10., 10., 10., 10.,\n",
       "       11., 11., 11., 11., 12., 12., 12., 12., 13., 13., 13., 13., 14.,\n",
       "       14., 14., 14., 15., 15., 15., 15., 16., 16., 16., 16., 17., 17.,\n",
       "       17., 17., 18., 18., 18., 18., 19., 19., 19., 19., 20., 20., 20.,\n",
       "       20., 21., 21., 21., 21., 22., 22., 22., 22., 23., 23., 23., 23.,\n",
       "       24., 24., 24., 24., 25., 25., 25., 25., 26., 26., 26., 26., 27.,\n",
       "       27., 27., 27., 28., 28., 28., 28., 29., 29., 29., 29., 30., 30.,\n",
       "       30., 30., 31., 31., 31., 31., 32., 32., 32., 32., 33., 33., 33.,\n",
       "       33., 34., 34., 34., 34., 35., 35., 35., 35., 36., 36., 36., 36.,\n",
       "       37., 37., 37., 37., 38., 38., 38., 38., 39., 39., 39., 39., 40.,\n",
       "       40., 40., 40., 41., 41., 41., 41., 42., 42., 42., 42., 43., 43.,\n",
       "       43., 43., 44., 44., 44., 44., 45., 45., 45., 45., 46., 46., 46.,\n",
       "       46., 47., 47., 47., 47., 48., 48., 48., 48., 49., 49., 49., 49.,\n",
       "       50., 50., 50., 50., 51., 51., 51., 51., 52., 52., 52., 52., 53.,\n",
       "       53., 53., 53., 54., 54., 54., 54., 55., 55., 55., 55., 56., 56.,\n",
       "       56., 56., 57., 57., 57.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(bdata.dataset.shape) #(number of scan, number of voxels)\n",
    "# bdata.dataset[:,300334] \n",
    "block = bdata.get('Block').flatten()\n",
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed70483-b4ce-43c8-9e55-5f757cb1e47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
