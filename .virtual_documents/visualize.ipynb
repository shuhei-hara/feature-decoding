import sqlite3
import pandas as pd
from bdpy.dataform import SQLite3KeyValueStore
import numpy as np



all_subject_data = []
# for subject in ['DI',  'FA',  'HH' , 'HK' , 'HM' , 'KH' , 'KT',  'MF' , 'MN' , 'MOt' , 'MY',  'MYa' , 'NK' ,'NKu' , 'RM', 'SK' , 'SY' , 'TK' , 'TN' , 'TY',  'YA']:
for subject in ['DI',  'FA',  'HH' , 'HK' , 'HM' , 'KH' , 'KT',  'MF' , 'MN' , 'MOt' , 'MY',  'MYa' , 'NK' ,'NKu' , 'RM', 'SK' , 'SY' , 'TK' , 'TN' , 'TY',  'YA']:
    dirr = f'/flash/DoyaU/shuhei/decoding_train_cv/Alexnet/{subject}_evaluation.db'
    db = SQLite3KeyValueStore(dirr)
    result_list = []
    for roi in ['V1','V2','V3','V4','LOC','PPA','FFA']:
        for layer in ['conv1','conv2','conv3','conv4','conv5','fc6','fc7','fc8']:
            pattern_correlation = db.get(layer=layer, subject=f'sub-{subject}', roi=roi, metric='pattern_correlation')
            temp_df = pd.DataFrame({
                'ROI': [roi]* len(pattern_correlation),
                'profile_correlation': pattern_correlation,
                'Layer': [layer]* len(pattern_correlation),
                'Subject': [subject] * len(pattern_correlation),
            })
            result_list.append(temp_df)

    # Combine the data for this subject
    subject_layer_data = pd.concat(result_list)
    all_subject_data.append(subject_layer_data)
    
df = pd.concat(all_subject_data, ignore_index=True)


import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.barplot(x='ROI', y='profile_correlation', hue='Layer', data=df, ci=None, palette='muted')

# plt.title('each image')
# plt.xticks(rotation=45)
# plt.tight_layout()
plt.show()

group1_subjects = ['DI', 'TT', 'HM', 'RM', 'KH', 'MF', 'FA', 'MOt', 'KT', 'SY', 'TY']
df['Group'] = df['Subject'].apply(lambda x: 'Schizophrenia' if x in group1_subjects else 'Control')


layers = np.unique(df['Layer'])

fig, axes = plt.subplots(8, 1, figsize=(8, 15), sharex=True, sharey=True)

for i, layer in enumerate(layers):
    layer_data = df[df['Layer'] == layer]
    
    sns.barplot(
        x='ROI', y='pattern_correlation', hue='Group', 
        data=layer_data, ax=axes[i], errorbar=None
    )
    
    axes[i].set_title(layer, fontsize=15)
    axes[i].set_ylim(0, 0.4)
    axes[i].legend([], [], frameon=False)  # Remove legend for each subplot

    plt.setp(axes[i].get_xticklabels(), fontsize=16)

# Set common labels and legend
fig.suptitle('Each Image', fontsize=14)

# Add a single legend for the entire figure
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right', title='Group', fontsize=10)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()


all_subject_data = []
for subject in ['DI',  'FA',  'HH' , 'HK' , 'HM' , 'KH' , 'KT',  'MF' , 'MN' , 'MOt' , 'MY',  'MYa' , 'NK' ,'NKu' , 'RM', 'SK' , 'SY' , 'TK' , 'TN' , 'TY',  'YA']:
    dirr = f'/flash/DoyaU/shuhei/decoding_train_cv/Alexnet/{subject}_evaluation.db'
    db = SQLite3KeyValueStore(dirr)
    result_list = []
    for roi in ['V1','V2','V3','V4','LOC','PPA','FFA']:
        for layer in ['conv1','conv2','conv3','conv4','conv5','fc6','fc7','fc8']:
            unit_correlation = db.get(layer=layer, subject=f'sub-{subject}', roi=roi, metric='unit_correlation')
            temp_df = pd.DataFrame({
                'ROI': [roi]* len(unit_correlation),
                'unit_correlation': unit_correlation,
                'Layer': [layer]* len(unit_correlation),
                'Subject': [subject] * len(unit_correlation),
            })
            result_list.append(temp_df)

    # Combine the data for this subject
    subject_layer_data = pd.concat(result_list)
    all_subject_data.append(subject_layer_data)
    
df = pd.concat(all_subject_data, ignore_index=True)


import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.barplot(x='ROI', y='unit_correlation', hue='Layer', data=df, ci=None, palette='muted')

# plt.title('each image')
# plt.xticks(rotation=45)
# plt.tight_layout()
plt.show()


group1_subjects = ['DI', 'TT', 'HM', 'RM', 'KH', 'MF', 'FA', 'MOt', 'KT', 'SY', 'TY']
df['Group'] = df['Subject'].apply(lambda x: 'Schizophrenia' if x in group1_subjects else 'Control')


layers = np.unique(df['Layer'])

fig, axes = plt.subplots(8, 1, figsize=(8, 15), sharex=True, sharey=True)

for i, layer in enumerate(layers):
    layer_data = df[df['Layer'] == layer]
    
    sns.barplot(
        x='ROI', y='unit_correlation', hue='Group', 
        data=layer_data, ax=axes[i], errorbar=None
    )
    
    axes[i].set_title(layer, fontsize=15)
    axes[i].set_ylim(0, 0.6)
    axes[i].legend([], [], frameon=False)  # Remove legend for each subplot

    plt.setp(axes[i].get_xticklabels(), fontsize=16)

# Set common labels and legend
fig.suptitle('Each Image', fontsize=14)

# Add a single legend for the entire figure
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right', title='Group', fontsize=10)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()








all_subject_data = []
for subject in ['DI',  'FA',  'HH' , 'HK' , 'HM' , 'KH' , 'KT',  'MF' , 'MN' , 'MOt' , 'MY',  'MYa' , 'NK' ,'NKu' , 'RM', 'SK' , 'SY' , 'TK' , 'TN' , 'TY',  'YA']:
    dirr_nonblur = f'/flash/DoyaU/shuhei/decoding_alexnet/test_original_evaluation/{subject}_evaluation.db'
    dirr_blur = f'/flash/DoyaU/shuhei/decoding_alexnet/test_evaluation/{subject}_evaluation.db'
    db_nonblur = SQLite3KeyValueStore(dirr_nonblur)
    db_blur = SQLite3KeyValueStore(dirr_blur)
    result_list = []
    for roi in ['V1','V2','V3','V4','LOC','PPA','FFA']:
        for layer in ['conv1','conv2','conv3','conv4','conv5','fc6','fc7','fc8']:
            correlation_nonblulr = db_nonblur.get(layer=layer, subject=f'sub-{subject}', roi=roi, metric='pattern_correlation') # check metric
            correlation_blur = db_blur.get(layer=layer, subject=f'sub-{subject}', roi=roi, metric='pattern_correlation') # check metric
            temp_df = pd.DataFrame({
                'ROI': [roi]* len(correlation_nonblulr),
                'correlation_nonblur': correlation_nonblulr,
                'correlation_blur': correlation_blur,
                'Feature Gain': correlation_nonblulr - correlation_blur,
                'Layer': [layer]* len(correlation_nonblulr),
                'Subject': [subject] * len(correlation_nonblulr),
            })
            result_list.append(temp_df)

    # Combine the data for this subject
    subject_layer_data = pd.concat(result_list)
    all_subject_data.append(subject_layer_data)
    
df = pd.concat(all_subject_data, ignore_index=True)
group1_subjects = ['DI', 'TT', 'HM', 'RM', 'KH', 'MF', 'FA', 'MOt', 'KT', 'SY', 'TY']
df['Group'] = df['Subject'].apply(lambda x: 'Schizophrenia' if x in group1_subjects else 'Control')


import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.barplot(x='ROI', y='correlation_blur', hue='Layer', data=df, ci=None, palette='muted')

plt.show()


from scipy.stats import sem
layers = np.unique(df['Layer'])
fig, axes = plt.subplots(8, 1, figsize=(8, 15), sharex=True, sharey=True)

for i, layer in enumerate(layers[::-1]):
    layer_data = df[df['Layer'] == layer]
    
    mean_val = np.mean(layer_data['correlation_blur'])
    ci95 = 1.96 * sem(layer_data['correlation_blur'])
    
    axes[i].bar('Same Image', mean_val, yerr=ci95, width=0.1)
    
    axes[i].set_title(layer, fontsize=15)
    axes[i].set_ylim(0, 0.2)
    axes[i].legend([], [], frameon=False)  # Remove legend for each subplot

    plt.setp(axes[i].get_xticklabels(), fontsize=16)

# Set common labels and legend
fig.suptitle('Each Image', fontsize=14)

# Add a single legend for the entire figure
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right', title='Group', fontsize=10)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()





layers = np.unique(df['Layer'])

fig, axes = plt.subplots(8, 1, figsize=(8, 15), sharex=True, sharey=True)

for i, layer in enumerate(layers[::-1]):
    layer_data = df[df['Layer'] == layer]
    
    sns.barplot(
        x='ROI', y='Feature Gain', hue='Group', 
        data=layer_data, ax=axes[i], #errorbar=None
    )
    
    axes[i].set_title(layer, fontsize=15)
    axes[i].set_ylim(-0.07, 0.2)
    axes[i].legend([], [], frameon=False)  # Remove legend for each subplot

    plt.setp(axes[i].get_xticklabels(), fontsize=16)

# Set common labels and legend
fig.suptitle('Each Image', fontsize=14)

# Add a single legend for the entire figure
handles, labels = axes[0].get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right', title='Group', fontsize=10)

plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()


df.head
# add corresponding image number
# 今度対応する画像番号を取ってくるようにする！！





df_list = []

# subject_list = ['DI','HM','RM','KH','MF', 'MOt','FA','KT','SY','TY',
#           'MN','NK','SK','NKu','MY','YA','TK','TN','HH','MYa','HK']
subject_list = ['DI','HM','RM','KH','MF', 'MOt','FA','KT','SY','TY',
          'MN','NK','SK','NKu','MY','YA','TK','TN','HH','MYa','HK']

df_image = pd.DataFrame(index=['subject', 'CNN_layer','prior']).T

for subject in subject_list:

    CNNfeature_blur = make_feature(subject,blur=True)
    CNNfeature_nonblur = make_feature(subject,blur=False)
    

    fMRIresponse = np.load(f'/flash/DoyaU/shuhei/fmri_decoding/fmri_test/{subject}_fmriresponse.npz')

    files = sorted(glob.glob('/bucket/DoyaU/Shuhei/cat_fox/behavior/Response/'+subject+'/*'))
    likelihood_dir = '/bucket/DoyaU/Shuhei/cat_fox/behavior/online_likelihood.csv'
    like_data = pd.read_csv(likelihood_dir)
    
    prior_array = np.zeros((100,2))
    imgseq_array = np.zeros((100,1))
    unit_array = np.zeros((1000,1))
    lik_array = np.zeros((100,1))
    for n,f in enumerate(files):
        data =scipy.io.loadmat(f)
        for a,i in enumerate(np.squeeze(data['StimPriorRun'])):
            prior_array[(20*n)+a , :] = np.squeeze(i)
            imgseq_array[(20*n):(20*(n+1)), :] = data['ImgSeq']
            
        for i,im_seq in enumerate(data['ImgSeq']):
            lik_array[(20*n)+i, :] = like_data[like_data['im_number']==im_seq[0]]['categ1_ave']

    lik_levels = np.array([categorize_lik(val) for val in lik_array])
    
    for index_CNNlayer in range(numCNNlayer):
    
        
        y_blur = CNNfeature_blur[:,index_CNNlayer,:]
        y_nonblur = CNNfeature_nonblur[:,index_CNNlayer,:]
        
        for index_roi,roi in enumerate(rois):
    
            #show progress
            print("Subject: " + subject)
            print("CNN layer #: " +str((index_CNNlayer)+1))
            print('ROI: ' + roi)
            
            
            # Take ROI part
            fMRIresponse_roi = fMRIresponse[roi]  
        
            model_dir = os.path.join(models_dir_root , subject, 'layer'+str(index_CNNlayer+1), roi)
            # print(model_dir)
            x_mean = load_array(os.path.join(model_dir, 'x_mean.mat'), key='x_mean')  # shape = (1, n_voxels)
            x_norm = load_array(os.path.join(model_dir, 'x_norm.mat'), key='x_norm')  # shape = (1, n_voxels)
            y_mean = load_array(os.path.join(model_dir, 'y_mean.mat'), key='y_mean')  # shape = (1, shape_features)
            y_norm = load_array(os.path.join(model_dir, 'y_norm.mat'), key='y_norm')  # shape = (1, shape_features)
            # print(x_mean)
            x = fMRIresponse_roi
            x = (x - x_mean) / x_norm
    
            
            model = FastL2LiR()
        
            test = ModelTest(model, x)
            test.model_format = 'bdmodel'
            test.model_path = model_dir
            test.dtype = np.float32
            # test.chunk_axis = chunk_axis
        
            y_pred = test.run()
            y_pred = y_pred * y_norm + y_mean

            
            # Sorting indices based on image_info
            indices_50 = np.where(np.max(prior_array, axis=1) == 50)[0]
            indices_70 = np.where(np.max(prior_array, axis=1) == 70)[0]
            indices_90 = np.where(np.max(prior_array, axis=1) == 90)[0]
            
            y_pred_50 = y_pred[indices_50]
            y_pred_70 = y_pred[indices_70]
            y_pred_90 = y_pred[indices_90]

            y_blur_50 = y_blur[indices_50]
            y_blur_70 = y_blur[indices_70]
            y_blur_90 = y_blur[indices_90]

            y_nonblur_50 = y_nonblur[indices_50]
            y_nonblur_70 = y_nonblur[indices_70]
            y_nonblur_90 = y_nonblur[indices_90]

            
            corr_unit_blur_50 = bdpy.stats.corrcoef(y_pred_50,y_blur_50,var='col')
            corr_unit_blur_70 = bdpy.stats.corrcoef(y_pred_70,y_blur_70,var='col')
            corr_unit_blur_90 = bdpy.stats.corrcoef(y_pred_90,y_blur_90,var='col')
            
            corr_unit_nonblur_50 = bdpy.stats.corrcoef(y_pred_50,y_nonblur_50,var='col')
            corr_unit_nonblur_70 = bdpy.stats.corrcoef(y_pred_70,y_nonblur_70,var='col')
            corr_unit_nonblur_90 = bdpy.stats.corrcoef(y_pred_90,y_nonblur_90,var='col')

            
            temp_df_img = pd.DataFrame({
                'subject': [subject] * len(unit_array),
                'CNN_layer': [index_CNNlayer] * len(unit_array), 
                'ROI': [roi] * len(unit_array), 
                'corr_unit_blur': corr_unit_blur_50,
                'corr_unit_nonblur': corr_unit_nonblur_50,
                'feature_gain': corr_unit_nonblur_50 - corr_unit_blur_50,
                'prior': 'Low',
            })
    
            df_image = pd.concat([df_image, temp_df_img], ignore_index=True)
            
            temp_df_img = pd.DataFrame({
                'subject': [subject] * len(unit_array),
                'CNN_layer': [index_CNNlayer] * len(unit_array), 
                'ROI': [roi] * len(unit_array), 
                'corr_unit_blur': corr_unit_blur_70,
                'corr_unit_nonblur': corr_unit_nonblur_70,
                'feature_gain': corr_unit_nonblur_70 - corr_unit_blur_70,
                'prior': 'Intermediate',
            })
    
            df_image = pd.concat([df_image, temp_df_img], ignore_index=True)
            temp_df_img = pd.DataFrame({
                'subject': [subject] * len(unit_array),
                'CNN_layer': [index_CNNlayer] * len(unit_array), 
                'ROI': [roi] * len(unit_array), 
                'corr_unit_blur': corr_unit_blur_90,
                'corr_unit_nonblur': corr_unit_nonblur_90,
                'feature_gain': corr_unit_nonblur_90 - corr_unit_blur_90,
                'prior': 'High',
            })
    
            df_image = pd.concat([df_image, temp_df_img], ignore_index=True)



# db_nonblur._get_keys()
print(db_nonblur.get(layer='fc6', subject='sub-DI', roi='LOC', metric='pattern_correlation'))



