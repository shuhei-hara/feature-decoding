'''DNN Feature decoding (corss-validation) - decoders training script.'''


from typing import Dict, List, Optional

from itertools import product
import os
import shutil
from time import time
import warnings
import pickle

import bdpy
import bdpy.stats
from bdpy.bdata.utils import select_data_multi_bdatas, get_labels_multi_bdatas
from bdpy.dataform import Features, save_array
from bdpy.dataform.utils import get_multi_features
from bdpy.distcomp import DistComp
from bdpy.ml import ModelTraining
from bdpy.ml.crossvalidation import make_cvindex_generator
from bdpy.pipeline.config import init_hydra_cfg
from bdpy.util import makedir_ifnot
from fastl2lir import FastL2LiR
import numpy as np
import yaml
import scipy.stats


import sys
from hydra.core.global_hydra import GlobalHydra

# Clear existing Hydra instance if already initialized
GlobalHydra.instance().clear()

# Simulate command-line arguments
sys.argv = ['notebook', 'config/cv.yaml']  # Adjust the path to your actual config

# Initialize config
cfg = init_hydra_cfg()

subject = 'DI'

analysis_name = cfg["_run_"]["name"] + '-' + cfg["_run_"]["config_name"]


training_fmri = {
    subject["name"]: subject["paths"]
    for subject in cfg["decoder"]["fmri"]["subjects"]
}
rois = {
    roi["name"]: roi["select"]
    for roi in cfg["decoder"]["fmri"]["rois"]
}
num_voxel = {
    roi["name"]: roi["num"]
    for roi in cfg["decoder"]["fmri"]["rois"]
}
label_key = cfg["decoder"]["fmri"]["label_key"]

training_features = cfg["decoder"]["features"]["paths"]
layers = cfg["decoder"]["features"]["layers"]
feature_index_file = cfg.decoder.features.get("index_file", None)


decoder_dir = cfg["decoder"]["path"]

cv_folds = cfg.cv.get("folds", None)
cv_exclusive = cfg.cv.get("exclusive_key", None)



fmri_data = training_fmri
features_paths = training_features
output_dir=decoder_dir
rois=rois
num_voxel=num_voxel
label_key=label_key
cv_key=cfg["cv"]["key"]
cv_folds=cv_folds
# cv_exclusive=cv_exclusive,
cv_exclusive=None
layers=layers
feature_index_file=feature_index_file
alpha=cfg["decoder"]["parameters"]["alpha"]
chunk_axis=cfg["decoder"]["parameters"]["chunk_axis"]
analysis_name=analysis_name
subject=subject
    

def unwrap(x):
    while isinstance(x, (tuple, list)):
        x = x[0]
    return x

fmri_data = unwrap(fmri_data)
rois = unwrap(rois)
num_voxel = unwrap(num_voxel)
label_key = unwrap(label_key)
cv_folds = unwrap(cv_folds)
layers = unwrap(layers)
feature_index_file = unwrap(feature_index_file)
analysis_name = unwrap(analysis_name)


data_brain = {sbj: [bdpy.BData(f) for f in data_files] for sbj, data_files in fmri_data.items()}
data_brain = {f'sub-{subject}': data_brain[f'sub-{subject}']}

# bdatadayo = data_brain[f'sub-{subject}'][0]

# print("Dataset shape:", bdatadayo.dataset.shape)
# print("First few rows of voxel data:\n", bdatadayo.dataset[:5])
# bdatadayo.show_metadata()
# print("Run info:", bdatadayo.get_metadata("Block"))  # Replace with actual key from show_metadata()

# if "ROI_standard_V1" in bdatadayo.metadata.key:
#     v1_data = bdatadayo.select("ROI_standard_V1 = 1")
#     print("V1 voxel data shape:", v1_data.shape)


if feature_index_file is not None:
    #(1000,1000)この時点ではまだどの画像が選ばれているかわかっていない。
    #どこの1000ユニットを取るかを決めているだけ。
    data_features = [Features(f, feature_index=os.path.join(f, feature_index_file)) for f in features_paths] 
else:
    data_features = [Features(f) for f in features_paths]

# print(data_features[0].get_features('conv1').shape)
# print(data_features)

if feature_index_file is not None:
    feature_index_save_file = os.path.join(output_dir, 'feature_index.mat')
    shutil.copy(feature_index_file, feature_index_save_file)
    print('Saved %s' % feature_index_save_file)


# makedir_ifnot('./tmp')
# distcomp_db = os.path.join('./tmp', analysis_name + '.db')
# distcomp = DistComp(backend='sqlite3', db_path=distcomp_db)


sbj = f'sub-{subject}'
for layer, roi in product(layers, rois.keys()):
    layer = 'fc8'
    roi = 'LOC'
    
    print('--------------------')
    print('Layer:      %s' % layer)
    print('Subject:    %s' % sbj)
    print('ROI:        %s' % roi)
    print('Num voxels: %d' % num_voxel[roi])

    # FXIME: support multiple datasets
    cv_index = make_cvindex_generator(
        data_brain[sbj][0].select(cv_key),
        folds=cv_folds,
        exclusive=cv_exclusive
    )
    cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]
    
    for cv_label, (train_index, test_index) in zip(cv_labels, cv_index):
        
        # print('CV fold: {} ({} training; {} test)'.format(cv_label, len(train_index), len(test_index)))

        analysis_id = analysis_name + '-' + sbj + '-' + roi + '-' + cv_label + '-' + layer
        model_dir   = os.path.join(output_dir, layer, sbj, roi, cv_label, 'model')

        makedir_ifnot(model_dir)

        # Check whether the analysis has been done or not.
        # info_file = os.path.join(model_dir, 'info.yaml')
        # if os.path.exists(info_file):
        #     with open(info_file, 'r') as f:
        #         info = yaml.safe_load(f)
        #     while info is None:
        #         warnings.warn('Failed to load info from %s. Retrying...'
        #                       % info_file)
        #         with open(info_file, 'r') as f:
        #             info = yaml.safe_load(f)
        #     if '_status' in info and 'computation_status' in info['_status']:
        #         if info['_status']['computation_status'] == 'done':
        #             print('%s is already done and skipped' % analysis_id)
        #             continue
                    
        start_time = time()

        brain = select_data_multi_bdatas(data_brain[sbj], rois[roi]) #(750, 5093) ROIのfMRIデータ
        brain_labels = get_labels_multi_bdatas(data_brain[sbj], label_key) #順番通り画像データが750取れているね

        feat_labels = brain_labels
        feat = get_multi_features(data_features, layer, labels=feat_labels) #画像順にfeatureが取れているはず
        feat = scipy.stats.zscore(feat,axis=0)
        
        brain = brain[train_index, :] #これはただtain_indexに沿ってtrainingデータをデータをとってきている
        brain_labels = np.array(brain_labels)[train_index]#これはただtain_indexに沿ってtrainingデータをデータをとってきている

        feat = feat[train_index, :]
        feat_labels = np.array(feat_labels)[train_index]
        
        # Get training samples of Y for get mean and norm parameters
        feat_train = feat

        # Normalize X (fMRI data)
        brain_mean = np.mean(brain, axis=0)[np.newaxis, :]  # np.newaxis was added to match Matlab outputs
        brain_norm = np.std(brain, axis=0, ddof=1)[np.newaxis, :]

        # Normalize Y (DNN features)
        feat_mean = np.mean(feat_train, axis=0)[np.newaxis, :]
        feat_norm = np.std(feat_train, axis=0, ddof=1)[np.newaxis, :]

        norm_param = {
            'x_mean': brain_mean, 'y_mean': feat_mean,
            'x_norm': brain_norm, 'y_norm': feat_norm
        }
        save_targets = [u'x_mean', u'y_mean', u'x_norm', u'y_norm']
        for sv in save_targets:
            save_file = os.path.join(model_dir, sv + '.mat')
            if not os.path.exists(save_file):
                try:
                    save_array(save_file, norm_param[sv], key=sv, dtype=np.float32, sparse=False)
                    print('Saved %s' % save_file)
                except Exception:
                    warnings.warn('Failed to save %s. Possibly double running.' % save_file)

        # Preparing learning
        # ------------------
        model = FastL2LiR()
        model_param = {
            'alpha':  alpha,
            'n_feat': num_voxel[roi],
            'dtype':  np.float32
        }


        print('Model training')
        start_time = time()

        train = ModelTraining(model, brain, feat_train) # なんでここがfeatなのかわからない、たぶんfeat_trianだと思う。
        train.id = analysis_id
        train.model_parameters = model_param

        train.X_normalize = {'mean': brain_mean,
                             'std': brain_norm}
        train.Y_normalize = {'mean': feat_mean,
                             'std': feat_norm}
        # train.Y_sort = {'index': feat_index}

        train.dtype = np.float32
        train.chunk_axis = chunk_axis
        train.save_format = 'bdmodel'
        train.save_path = model_dir
        # train.distcomp = distcomp

        train.run()

        print('Total elapsed time (model training): %f' % (time() - start_time))

        break
    break








from bdpy.dataform import load_array, save_array
from bdpy.ml import ModelTest
average_sample = False
decoder_path='/flash/DoyaU/shuhei/decoding_train_cv'
excluded_labels=None

# Print info -------------------------------------------------------------
print('Subjects:        %s' % list(fmri_data.keys()))
print('ROIs:            %s' % list(rois.keys()))
print('Decoders:        %s' % decoder_path)
print('Layers:          %s' % layers)
print('CV:              %s' % cv_key)
print('')

# Load data --------------------------------------------------------------
print('----------------------------------------')
print('Loading data')

# FIXME: support multiple datasets
data_brain = {
    sbj: bdpy.BData(dat_file[0])
    for sbj, dat_file in fmri_data.items()
}
data_brain = {f'sub-{subject}': data_brain[f'sub-{subject}']}

# Initialize directories -------------------------------------------------
output_dir = '/flash/DoyaU/shuhei/decoding_train_cv/Alexnet'
makedir_ifnot(output_dir)
makedir_ifnot('tmp')

# Save feature index -----------------------------------------------------
if feature_index_file is not None:
    feature_index_save_file = os.path.join(output_dir, 'feature_index.mat')
    shutil.copy(feature_index_file, feature_index_save_file)
    print('Saved %s' % feature_index_save_file)

# Distributed computation setup ------------------------------------------
distcomp_db = os.path.join('./tmp', analysis_name + '.db')
distcomp = DistComp(backend='sqlite3', db_path=distcomp_db)

# Analysis loop ----------------------------------------------------------
print('----------------------------------------')
print('Analysis loop')

sbj = f'sub-{subject}'
for layer, roi in product(layers, rois):
    layer = 'fc8'
    roi = 'LOC'
    print('--------------------')
    print('Layer:   %s' % layer)
    print('Subject: %s' % sbj)
    print('ROI:     %s' % roi)

    cv_index = make_cvindex_generator(
        data_brain[sbj].select(cv_key),
        folds=cv_folds,
        exclusive=None
    )

    if 'name' in cv_folds[0]:
        cv_labels = ['cv-{}'.format(cv['name']) for cv in cv_folds]
    else:
        cv_labels = ['cv-fold{}'.format(icv + 1) for icv in range(len(cv_folds))]

    for cv_label, (train_index, test_index) in zip(cv_labels, cv_index):
        print('CV fold: {} ({} training; {} test)'.format(cv_label, len(train_index), len(test_index)))

        # Setup
        # -----
        analysis_id = analysis_name + '-' + sbj + '-' + roi + '-' + cv_label + '-' + layer
        decoded_feature_dir = os.path.join(output_dir, layer, sbj, roi, cv_label, 'decoded_features')

        # if os.path.exists(decoded_feature_dir):
        #     print('%s is already done. Skipped.' % analysis_id)
        #     continue

        makedir_ifnot(decoded_feature_dir)

        # if not distcomp.lock(analysis_id):
        #     print('%s is already running. Skipped.' % analysis_id)
        #     continue

        # Preparing data
        # --------------
        print('Preparing data')

        start_time = time()

        # Brain data
        brain = data_brain[sbj].select(rois[roi]) #(750, 5093) ROIのfMRIデータ
        brain_labels = data_brain[sbj].get_label(label_key) #順番通り画像データが750取れているね

        # Extract test data
        brain = brain[test_index, :]
        brain_labels = np.array(brain_labels)[test_index]

        print('Elapsed time (data preparation): %f' % (time() - start_time))

        # Model directory
        # ---------------
        model_dir = os.path.join(decoder_path, layer, sbj, roi, cv_label, 'model')

        # Preprocessing
        # -------------
        brain_mean = load_array(os.path.join(model_dir, 'x_mean.mat'), key='x_mean')  # shape = (1, n_voxels)
        brain_norm = load_array(os.path.join(model_dir, 'x_norm.mat'), key='x_norm')  # shape = (1, n_voxels)
        feat_mean = load_array(os.path.join(model_dir, 'y_mean.mat'), key='y_mean')  # shape = (1, shape_features)
        feat_norm = load_array(os.path.join(model_dir, 'y_norm.mat'), key='y_norm')  # shape = (1, shape_features)

        brain = (brain - brain_mean) / brain_norm

        # Prediction
        # ----------
        print('Prediction')

        start_time = time()

        model = FastL2LiR()

        test = ModelTest(model, brain)
        test.model_format = 'bdmodel'
        test.model_path = model_dir
        test.dtype = np.float32
        test.chunk_axis = chunk_axis

        feat_pred = test.run()

        print('Total elapsed time (prediction): %f' % (time() - start_time))

        # Postprocessing
        # --------------
        feat_pred = feat_pred * feat_norm + feat_mean
        print(feat_pred)

        # Save results
        # ------------
        print('Saving results')

        start_time = time()

        # Predicted features
        for i, label in enumerate(brain_labels):
            # Predicted features
            _feat = np.array([feat_pred[i,]])  # To make feat shape 1 x M x N x ...

            # Save file name
            save_file = os.path.join(decoded_feature_dir, '%s.mat' % label)

            # Save
            save_array(save_file, _feat, key='feat', dtype=np.float32, sparse=False)

        print('Saved %s' % decoded_feature_dir)

        print('Elapsed time (saving results): %f' % (time() - start_time))

        distcomp.unlock(analysis_id)

        break
    break









from bdpy.dataform import DecodedFeatures, SQLite3KeyValueStore
import hdf5storage
from bdpy.evals.metrics import profile_correlation, pattern_correlation, pairwise_identification


class ResultsStore(SQLite3KeyValueStore):
    """Results store for feature decoding evaluation."""
    pass

data_brain = {sbj: [bdpy.BData(f) for f in data_files] for sbj, data_files in fmri_data.items()}
data_brain = {f'sub-{subject}': data_brain[f'sub-{subject}']}
brain_labels = get_labels_multi_bdatas(data_brain[sbj], label_key)

rois = {
    roi["name"]: roi["select"]
    for roi in cfg["decoder"]["fmri"]["rois"]
}

decoded_feature_path = '/flash/DoyaU/shuhei/decoding_train_cv/Alexnet'
true_feature_path = '/flash/DoyaU/shuhei/bdata_decoding/features'
# output_file_fold=os.path.join(decoded_feature_path, 'evaluation_fold.db')
# output_file_pooled=os.path.join(decoded_feature_path, 'evaluation.db')
feature_decoder_path='/flash/DoyaU/shuhei/decoding_train_cv'

# Loading data ###########################################################

# True features
#(1000,1000)この時点ではまだどの画像が選ばれているかわかっていない。
#どこの1000ユニットを取るかを決めているだけ。
features_test = Features(true_feature_path, feature_index=feature_index_file)
print(features_test.get_features('conv1').shape) 

# Decoded features

decoded_features = DecodedFeatures(decoded_feature_path)

cv_folds = decoded_features.folds

# # Metrics ################################################################
metrics = ['profile_correlation', 'pattern_correlation', 'identification_accuracy']
pooled_operation = {
    "profile_correlation": "mean",
    "pattern_correlation": "concat",
    "identification_accuracy": "concat",
}

# # Evaluating decoding performances #######################################

# if os.path.exists(output_file_fold):
#     print('Loading {}'.format(output_file_fold))
#     results_db = ResultsStore(output_file_fold)
# else:
#     print('Creating new evaluation result store')
#     keys = ["layer", "subject", "roi", "fold", "metric"]
#     results_db = ResultsStore(output_file_fold, keys=keys)

true_labels = features_test.labels #(1000)  ここはまずめちゃくちゃ変えなきゃいけない。


# for layer in layers:
# layer='conv1'
layer = 'fc8'
true_y = features_test.get_features(layer=layer)
true_y = scipy.stats.zscore(true_y,axis=0)

for roi, fold in list(product(rois, cv_folds)):
    roi = 'LOC'
    print('Subject: {} - ROI: {} - Fold: {}'.format(subject, roi, fold))

    # Check if the evaluation is already done
    # exists = True
    # for metric in metrics:
    #     exists = exists and results_db.exists(layer=layer, subject=subject, roi=roi, fold=fold, metric=metric)
    # if exists:
    #     print('Already done. Skipped.')
    #     continue

    #むしろここの順番がおかしいから後もおかしい。
    pred_y = decoded_features.get(layer=layer, subject='sub-DI', roi=roi, fold=fold)
    pred_labels = np.array(decoded_features.selected_label) #ここの順番が変なのがわるいみたいだ
    # pred_labels = np.array(brain_labels[:50]) #これをfold数によって変更させる。
    
    # if not average_sample:
    #     pred_labels = [re.match('trial_\d*-(.*)', x).group(1) for x in pred_labels]

    if not np.array_equal(pred_labels, true_labels):
        y_index = [np.where(np.array(true_labels) == x)[0][0] for x in pred_labels]
        true_y_sorted = true_y[y_index]
    else:
        true_y_sorted = true_y
    
    # Load Y mean and SD
    # Proposed by Ken Shirakawa. See https://github.com/KamitaniLab/brain-decoding-cookbook/issues/13.
    norm_param_dir = os.path.join(
        feature_decoder_path,
        layer, 'sub-DI', roi, fold,
        'model'
    )

    train_y_mean = hdf5storage.loadmat(os.path.join(norm_param_dir, 'y_mean.mat'))['y_mean']
    train_y_std = hdf5storage.loadmat(os.path.join(norm_param_dir, 'y_norm.mat'))['y_norm']

    # Evaluation ---------------------------


    # Pattern correlation
    # if not results_db.exists(layer=layer, subject=subject, roi=roi, fold=fold, metric='pattern_correlation'):
    # results_db.set(layer=layer, subject=subject, roi=roi, fold=fold, metric='pattern_correlation', value=np.array([]))
    r_patt = pattern_correlation(pred_y, true_y_sorted, mean=train_y_mean, std=train_y_std)
    # results_db.set(layer=layer, subject=subject, roi=roi, fold=fold, metric='pattern_correlation', value=r_patt)
    print('Mean pattern correlation:     {}'.format(np.nanmean(r_patt)))

    #for each image, corr_unit, over all images 1000dim, resulting in 750 images
    x = pred_y
    y = true_y_sorted
    
    m = train_y_mean.reshape(-1)
    s = train_y_std.reshape(-1)

    x = (x - m) / s
    y = (y - m) / s
    
    corr_image = np.array([np.corrcoef(x[i, :].ravel(), y[i, :].ravel())[0, 1] for i in range(pred_y.shape[0])])
    print('Mean decoding accuracy (corr_image):' + str(np.mean(corr_image)))
    break
    
print('All fold done')

# Pooled accuracy
# if os.path.exists(output_file_pooled):
#     print('Loading {}'.format(output_file_pooled))
#     pooled_db = ResultsStore(output_file_pooled)
# else:
#     print('Creating new evaluation result store')
#     keys = ["layer", "subject", "roi", "metric"]
#     pooled_db = ResultsStore(output_file_pooled, keys=keys)

# done_all = True  # Flag indicating that all conditions have been pooled
# for metric in metrics:
#     # Check if pooling is done
#     if pooled_db.exists(layer=layer, subject=subject, roi=roi, metric=metric):
#         continue
#     pooled_db.set(layer=layer, subject=subject, roi=roi, metric=metric, value=np.array([]))

#     # Check if all folds are complete
#     done = True
#     for fold in cv_folds:
#         if not results_db.exists(layer=layer, subject=subject, roi=roi, fold=fold, metric=metric):
#             done = False
#             break

#     # When all folds are complete, pool the results.
#     if done:
#         acc = []
#         for fold in cv_folds:
#             acc.append(results_db.get(layer=layer, subject=subject, roi=roi,
#                                       fold=fold, metric=metric))
#         if pooled_operation[metric] == "mean":
#             acc = np.nanmean(acc, axis=0)
#         elif pooled_operation[metric] == "concat":
#             acc = np.hstack(acc)
#         pooled_db.set(layer=layer, subject=subject, roi=roi,
#                       metric=metric, value=acc)

#     # If there are any unfinished conditions,
#     # do not pool the results and set the done_all flag to False.
#     else:
#         pooled_db.delete(layer=layer, subject=subject, roi=roi, metric=metric)
#         done_all = False
#         continue



pred_labels


import matplotlib.pyplot as plt
fig, axes = plt.subplots(1, 1, figsize=(15, 8), sharex=True, sharey=True)
plt.plot(pred_y[0,:100])
plt.plot(true_y_sorted[0,:100], '--')
plt.show()


plt.plot(pred_y[:,1])
plt.plot(true_y_sorted[:,1])
plt.show()


corr_unit.shape


x = pred_y
y = true_y_sorted


# Get num variables
nvar = x.shape[0]

# Get correlation
rmat = np.corrcoef(x, y, rowvar=1)
r = np.diag(rmat[:nvar, nvar:])
r


n_sample


_x



